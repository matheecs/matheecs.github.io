---
layout: post
title: "RL Notes"
categories: study
author: "Jixiang Zhang"
---

<p align="center">
  <img src="{{site.baseurl}}/images/rl.jpg" width="500"/>
</p>

<https://github.com/engineai-robotics/engineai_legged_gym>

### class LeggedRobotCfg

* env
* terrian
* init_state
* control
* sim
* viewer
* noise
* normalization
* commands
* asset
* domain_rand
* rewards

### class LeggedRobotCfgPPO

* policy
* runner
* algorithm

```text
[Rewards][ ✔ ] action_rate
[Rewards][ x ] action_smoothness
[Rewards][ x ] ang_vel_xy
[Rewards][ ✔ ] ankle_action_rate
[Rewards][ ✔ ] ankle_dof_acc
[Rewards][ x ] base_height
[Rewards][ ✔ ] collision
[Rewards][ ✔ ] dof_acc
[Rewards][ ✔ ] dof_pos_limits
[Rewards][ x ] dof_vel
[Rewards][ x ] feet_air_time
[Rewards][ ✔ ] feet_contact_forces
[Rewards][ ✔ ] feet_distance
[Rewards][ x ] feet_stumble
[Rewards][ ✔ ] lin_vel_z
[Rewards][ x ] no_fly
[Rewards][ ✔ ] orientation
[Rewards][ x ] stand_still
[Rewards][ x ] target_ankle_pos
[Rewards][ x ] target_hip_roll_pos
[Rewards][ ✔ ] target_joint_pos_l
[Rewards][ ✔ ] target_joint_pos_r
[Rewards][ ✔ ] termination
[Rewards][ ✔ ] torque_limits
[Rewards][ ✔ ] torques
[Rewards][ ✔ ] tracking_ang_vel
[Rewards][ x ] tracking_lin_vel
[Rewards][ ✔ ] tracking_lin_x_vel
[Rewards][ ✔ ] tracking_lin_y_vel
```

```bash
                     Learning iteration 20291/40000                     

                       Computation: 215205 steps/s (collection: 0.333s, learning 0.124s)
               Value function loss: 0.2949
                    Surrogate loss: -0.0028
             Mean action noise std: 0.51
                       Mean reward: 628.45
               Mean episode length: 1985.67
      Mean episode rew_action_rate: -1.6791
Mean episode rew_ankle_action_rate: -0.3818
    Mean episode rew_ankle_dof_acc: -0.0201
        Mean episode rew_collision: -0.0008
          Mean episode rew_dof_acc: -0.1378
   Mean episode rew_dof_pos_limits: -0.0000
Mean episode rew_feet_contact_forces: -0.2618
    Mean episode rew_feet_distance: 0.9603
        Mean episode rew_lin_vel_z: -0.0237
      Mean episode rew_orientation: 9.7049
Mean episode rew_target_joint_pos_l: 9.0367
Mean episode rew_target_joint_pos_r: 7.1679
      Mean episode rew_termination: -0.0000
    Mean episode rew_torque_limits: -0.0351
          Mean episode rew_torques: -0.1538
 Mean episode rew_tracking_ang_vel: 1.7254
Mean episode rew_tracking_lin_x_vel: 3.2664
Mean episode rew_tracking_lin_y_vel: 2.3819
```

<p align="center">
  <img src="{{site.baseurl}}/images/classes.png" width="500"/>
</p>

### class BaseTask

* get_observations
* get_privileged_observations
* render
* reset 重置所有并行仿真环境
* reset_idx **NotImplementedError**
* step **NotImplementedError**

### class LeggedRobot

* _compute_torques 用 Action + PD 算力矩
* _create_envs 根据 URDF 创建并行仿真环境
* _create_ground_plane
* _create_heightfield
* _create_trimesh
* _draw_debug_vis
* _get_env_origins
* _get_heights
* _get_noise_scale_vec 生成观测量噪声
* _init_buffers 初始化 Tensor 缓存变量
* _init_height_points
* _parse_cfg
* _post_physics_step_callback 指令采样 计算地形 外部干扰
* _prepare_reward_function
* _process_dof_props 获取非浮动基关节 Limit 信息
* _process_rigid_body_props 刚体质量 质心 惯量随机化
* _process_rigid_shape_props 摩擦系数随机化
* _push_robots 浮动基速度随机化
* _resample_commands
* _reset_dofs 非浮动基关节位置速度随机化
* _reset_root_states 浮动基位置速度随机化
* _update_terrain_curriculum
* check_termination 检查每个环境终止条件并重置
* compute_observations **override**
* compute_reward
* create_sim
* post_physics_step 检查终止条件 > 计算奖励 > 重置 > 计算观测量
* reset_idx **override**
* set_camera
* step **override**
* update_command_curriculum
* > _reward_lin_vel_z
* > _reward_ang_vel_xy
* > _reward_orientation
* > _reward_base_height
* > _reward_torques
* > _reward_dof_vel
* > _reward_dof_acc
* > _reward_action_rate
* > _reward_collision
* > _reward_termination
* > _reward_dof_pos_limits
* > _reward_dof_vel_limits
* > _reward_torque_limits
* > _reward_tracking_lin_vel **exp**
* > _reward_tracking_ang_vel **exp**
* > _reward_feet_air_time
* > _reward_stumble
* > _reward_stand_still
* > _reward_feet_contact_forces

### ZqSA01

* _get_noise_scale_vec **override**
* _refresh_gym_tensors
* _resample_commands **override**
* _reset_dofs **override**
* _reset_root_states **override**
* check_termination **override**
* compute_observations **override**
* compute_reference_states
* create_sim **override**
* reset_idx **override**
* step **override**
* > _reward_no_fly
* > _reward_target_joint_pos_l **exp**
* > _reward_target_joint_pos_r **exp**
* > _reward_orientation **exp**
* > _reward_tracking_lin_x_vel **exp**
* > _reward_tracking_lin_y_vel **exp**
* > _reward_action_smoothness
* > _reward_feet_distance **exp**
* > _reward_ankle_action_rate
* > _reward_ankle_dof_acc
* > _reward_target_ankle_pos **exp**
* > _reward_target_hip_roll_pos **exp**
